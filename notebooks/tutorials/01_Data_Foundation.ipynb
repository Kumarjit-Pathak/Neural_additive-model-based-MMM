{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20913f7",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Foundation for Marketing Mix Modeling\n",
    "## Understanding and Preparing Your Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the three key data sources for MMM\n",
    "- Learn how to aggregate data hierarchically\n",
    "- Master time-series data alignment\n",
    "- Create a unified dataset for modeling\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Foundation Matters\n",
    "\n",
    "Marketing Mix Modeling requires integrating multiple data sources at different granularities. The quality of your data pipeline directly impacts model accuracy. We'll work with:\n",
    "1. **Sales Data** (daily transactions)\n",
    "2. **Marketing Investment** (monthly budgets)\n",
    "3. **Brand Health Metrics** (NPS scores)\n",
    "\n",
    "Each source has different frequencies and hierarchies that we must carefully align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4edab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ba887",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Three Data Sources\n",
    "\n",
    "We have three critical data files that form the foundation of our MMM:\n",
    "- **firstfile.csv**: Daily sales transactions with product hierarchy\n",
    "- **MediaInvestment.csv**: Monthly marketing channel investments\n",
    "- **MonthlyNPSscore.csv**: Monthly Net Promoter Score (brand health indicator)\n",
    "\n",
    "Let's load and explore each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75916bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data():\n",
    "    '''\n",
    "    Load all three data sources and display their characteristics.\n",
    "    This function helps us understand what we're working with.\n",
    "    '''\n",
    "\n",
    "    # Load sales data - our primary response variable source\n",
    "    sales_df = pd.read_csv('data/firstfile.csv')\n",
    "    print(\"SALES DATA OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {sales_df.shape[0]:,} rows  {sales_df.shape[1]} columns\")\n",
    "    print(f\"Date range: {sales_df['Date'].min()} to {sales_df['Date'].max()}\")\n",
    "    print(f\"\\nColumns: {', '.join(sales_df.columns.tolist())}\")\n",
    "    print(f\"\\nProduct Categories: {sales_df['product_category'].nunique()}\")\n",
    "    print(f\"Product Subcategories: {sales_df['product_subcategory'].nunique()}\")\n",
    "\n",
    "    # Load marketing investment data - our key predictors\n",
    "    marketing_df = pd.read_csv('data/MediaInvestment.csv')\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MARKETING INVESTMENT DATA OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {marketing_df.shape[0]:,} rows  {marketing_df.shape[1]} columns\")\n",
    "\n",
    "    # Clean column names (remove any leading/trailing spaces)\n",
    "    marketing_df.columns = marketing_df.columns.str.strip()\n",
    "\n",
    "    # Identify marketing channels\n",
    "    marketing_channels = [col for col in marketing_df.columns\n",
    "                         if col not in ['Date', 'Total Investment']]\n",
    "    print(f\"Marketing Channels: {', '.join(marketing_channels)}\")\n",
    "    print(f\"Total Channels: {len(marketing_channels)}\")\n",
    "\n",
    "    # Load NPS data - brand health indicator\n",
    "    nps_df = pd.read_csv('data/MonthlyNPSscore.csv')\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"NPS (BRAND HEALTH) DATA OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {nps_df.shape[0]:,} rows  {nps_df.shape[1]} columns\")\n",
    "    print(f\"NPS Range: {nps_df['NPS'].min():.1f} to {nps_df['NPS'].max():.1f}\")\n",
    "\n",
    "    return sales_df, marketing_df, nps_df, marketing_channels\n",
    "\n",
    "# Load all data\n",
    "sales_df, marketing_df, nps_df, marketing_channels = load_and_explore_data()\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Sales Data:\")\n",
    "display(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a6e41",
   "metadata": {},
   "source": [
    "## Step 2: Data Quality Assessment\n",
    "\n",
    "Before merging, let's check for data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(sales_df, marketing_df, nps_df):\n",
    "    '''\n",
    "    Perform comprehensive data quality checks.\n",
    "    Good data quality is essential for reliable MMM results.\n",
    "    '''\n",
    "\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\n1. MISSING VALUES CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Sales Data:\")\n",
    "    sales_missing = sales_df.isnull().sum()\n",
    "    if sales_missing.sum() == 0:\n",
    "        print(\"  [OK] No missing values\")\n",
    "    else:\n",
    "        print(f\"   Missing values found:\")\n",
    "        print(sales_missing[sales_missing > 0])\n",
    "\n",
    "    print(\"\\nMarketing Data:\")\n",
    "    marketing_missing = marketing_df.isnull().sum()\n",
    "    if marketing_missing.sum() == 0:\n",
    "        print(\"  [OK] No missing values\")\n",
    "    else:\n",
    "        print(f\"   Missing values found:\")\n",
    "        print(marketing_missing[marketing_missing > 0])\n",
    "\n",
    "    # Check date formats\n",
    "    print(\"\\n2. DATE CONSISTENCY CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "    marketing_df['Date'] = pd.to_datetime(marketing_df['Date'])\n",
    "    nps_df['Date'] = pd.to_datetime(nps_df['Date'])\n",
    "\n",
    "    print(f\"Sales: {sales_df['Date'].min()} to {sales_df['Date'].max()}\")\n",
    "    print(f\"Marketing: {marketing_df['Date'].min()} to {marketing_df['Date'].max()}\")\n",
    "    print(f\"NPS: {nps_df['Date'].min()} to {nps_df['Date'].max()}\")\n",
    "\n",
    "    # Check for negative values\n",
    "    print(\"\\n3. NEGATIVE VALUES CHECK:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if (sales_df['GMV'] < 0).any():\n",
    "        print(\"   Negative GMV values found!\")\n",
    "    else:\n",
    "        print(\"  [OK] All GMV values are non-negative\")\n",
    "\n",
    "    if (sales_df['Units'] < 0).any():\n",
    "        print(\"   Negative Units found!\")\n",
    "    else:\n",
    "        print(\"  [OK] All Units are non-negative\")\n",
    "\n",
    "    # Check marketing spend\n",
    "    for channel in marketing_channels:\n",
    "        if channel in marketing_df.columns:\n",
    "            if (marketing_df[channel] < 0).any():\n",
    "                print(f\"   Negative spend in {channel}\")\n",
    "\n",
    "    return sales_df, marketing_df, nps_df\n",
    "\n",
    "# Run quality checks\n",
    "sales_df, marketing_df, nps_df = check_data_quality(sales_df, marketing_df, nps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266220e3",
   "metadata": {},
   "source": [
    "## Step 3: Hierarchical Data Aggregation\n",
    "\n",
    "A key insight for MMM is that marketing effects can vary by product hierarchy. We'll aggregate sales data by category and subcategory to capture these patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_aggregation(sales_df):\n",
    "    '''\n",
    "    Aggregate sales data by product hierarchy.\n",
    "    This captures how different product groups respond to marketing.\n",
    "\n",
    "    Why this matters:\n",
    "    - Premium products may respond differently to promotions\n",
    "    - Category-level effects can be stronger than individual SKUs\n",
    "    - Reduces noise while preserving important patterns\n",
    "    '''\n",
    "\n",
    "    print(\"Creating Hierarchical Aggregation...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Group by date and product hierarchy\n",
    "    hierarchy_agg = sales_df.groupby(\n",
    "        ['Date', 'product_category', 'product_subcategory']\n",
    "    ).agg({\n",
    "        'GMV': 'sum',           # Total revenue\n",
    "        'Units': 'sum',         # Total units sold\n",
    "        'Avg_MRP': 'mean',      # Average list price\n",
    "        'Avg_Price': 'mean'     # Average selling price\n",
    "    }).reset_index()\n",
    "\n",
    "    print(f\"\\nAggregation Results:\")\n",
    "    print(f\"  Original records: {len(sales_df):,}\")\n",
    "    print(f\"  Aggregated records: {len(hierarchy_agg):,}\")\n",
    "    print(f\"  Compression ratio: {len(sales_df)/len(hierarchy_agg):.1f}x\")\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    hierarchy_agg['Avg_Discount'] = hierarchy_agg['Avg_MRP'] - hierarchy_agg['Avg_Price']\n",
    "    hierarchy_agg['Discount_Pct'] = (hierarchy_agg['Avg_Discount'] /\n",
    "                                     (hierarchy_agg['Avg_MRP'] + 0.01)) * 100\n",
    "\n",
    "    # Show hierarchy structure\n",
    "    print(f\"\\nHierarchy Structure:\")\n",
    "    print(f\"  Categories: {hierarchy_agg['product_category'].nunique()}\")\n",
    "    print(f\"  Subcategories: {hierarchy_agg['product_subcategory'].nunique()}\")\n",
    "    print(f\"  Date range: {hierarchy_agg['Date'].nunique()} unique days\")\n",
    "\n",
    "    return hierarchy_agg\n",
    "\n",
    "# Create hierarchical aggregation\n",
    "hierarchy_data = create_hierarchical_aggregation(sales_df)\n",
    "\n",
    "# Visualize hierarchy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Sales by category\n",
    "plt.subplot(1, 2, 1)\n",
    "category_sales = hierarchy_data.groupby('product_category')['GMV'].sum().sort_values()\n",
    "category_sales.plot(kind='barh')\n",
    "plt.title('Total Sales by Product Category')\n",
    "plt.xlabel('GMV (Total)')\n",
    "plt.ylabel('Category')\n",
    "\n",
    "# Plot 2: Number of subcategories per category\n",
    "plt.subplot(1, 2, 2)\n",
    "subcat_count = hierarchy_data.groupby('product_category')['product_subcategory'].nunique()\n",
    "subcat_count.plot(kind='bar')\n",
    "plt.title('Subcategories per Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Subcategories')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a4c4e",
   "metadata": {},
   "source": [
    "## Step 4: Time-Series Alignment\n",
    "\n",
    "Marketing data is monthly but sales are daily. We need to align these different frequencies intelligently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_time_frequencies(hierarchy_data, marketing_df, nps_df):\n",
    "    '''\n",
    "    Align different time frequencies across data sources.\n",
    "\n",
    "    Strategy:\n",
    "    1. Keep sales at daily granularity (most detailed)\n",
    "    2. Interpolate monthly marketing to daily (smooth transitions)\n",
    "    3. Forward-fill NPS scores (brand perception changes slowly)\n",
    "\n",
    "    This preserves maximum information while ensuring alignment.\n",
    "    '''\n",
    "\n",
    "    print(\"Aligning Time Frequencies...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create daily date range\n",
    "    date_range = pd.date_range(\n",
    "        start=hierarchy_data['Date'].min(),\n",
    "        end=hierarchy_data['Date'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    print(f\"Daily date range: {len(date_range)} days\")\n",
    "    print(f\"From {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "    # Expand marketing data to daily\n",
    "    print(\"\\nExpanding Marketing Data (Monthly -> Daily):\")\n",
    "\n",
    "    # Create daily scaffold\n",
    "    marketing_daily = pd.DataFrame({'Date': date_range})\n",
    "    marketing_daily['YearMonth'] = marketing_daily['Date'].dt.to_period('M')\n",
    "    marketing_df['YearMonth'] = marketing_df['Date'].dt.to_period('M')\n",
    "\n",
    "    # Merge monthly data\n",
    "    marketing_daily = marketing_daily.merge(\n",
    "        marketing_df.drop('Date', axis=1),\n",
    "        on='YearMonth',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Interpolate marketing spend (linear interpolation for smooth transitions)\n",
    "    for channel in marketing_channels:\n",
    "        if channel in marketing_daily.columns:\n",
    "            # First forward fill to handle initial NaN\n",
    "            marketing_daily[channel] = marketing_daily[channel].fillna(method='ffill')\n",
    "            # Then interpolate for smooth transitions\n",
    "            marketing_daily[channel] = marketing_daily[channel].interpolate(method='linear')\n",
    "            # Finally fill any remaining NaN with 0\n",
    "            marketing_daily[channel] = marketing_daily[channel].fillna(0)\n",
    "\n",
    "    print(f\"  [OK] Interpolated {len(marketing_channels)} channels to daily\")\n",
    "\n",
    "    # Expand NPS to daily\n",
    "    print(\"\\nExpanding NPS Data (Monthly -> Daily):\")\n",
    "    nps_daily = pd.DataFrame({'Date': date_range})\n",
    "    nps_daily['YearMonth'] = nps_daily['Date'].dt.to_period('M')\n",
    "    nps_df['YearMonth'] = nps_df['Date'].dt.to_period('M')\n",
    "\n",
    "    nps_daily = nps_daily.merge(\n",
    "        nps_df[['YearMonth', 'NPS']],\n",
    "        on='YearMonth',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Forward fill NPS (assumption: brand perception changes slowly)\n",
    "    nps_daily['NPS'] = nps_daily['NPS'].fillna(method='ffill')\n",
    "    nps_daily['NPS'] = nps_daily['NPS'].fillna(nps_daily['NPS'].mean())\n",
    "\n",
    "    print(f\"  [OK] Expanded NPS to daily using forward-fill\")\n",
    "\n",
    "    return marketing_daily, nps_daily\n",
    "\n",
    "# Align time frequencies\n",
    "marketing_daily, nps_daily = align_time_frequencies(hierarchy_data, marketing_df, nps_df)\n",
    "\n",
    "# Visualize the alignment\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Original monthly marketing\n",
    "axes[0].plot(marketing_df['Date'], marketing_df['TV'], 'o-', label='Monthly TV Spend')\n",
    "axes[0].set_title('Original Monthly Marketing Data')\n",
    "axes[0].set_ylabel('Spend')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Interpolated daily marketing\n",
    "axes[1].plot(marketing_daily['Date'], marketing_daily['TV'], '-', label='Daily TV Spend (Interpolated)', alpha=0.7)\n",
    "axes[1].set_title('Interpolated Daily Marketing Data')\n",
    "axes[1].set_ylabel('Spend')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: NPS daily\n",
    "axes[2].plot(nps_daily['Date'], nps_daily['NPS'], '-', label='Daily NPS (Forward-filled)', color='green')\n",
    "axes[2].set_title('Daily NPS Scores')\n",
    "axes[2].set_ylabel('NPS')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839eca4",
   "metadata": {},
   "source": [
    "## Step 5: Creating the Unified Dataset\n",
    "\n",
    "Now we merge all aligned data sources into a single comprehensive dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c732f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_dataset(hierarchy_data, marketing_daily, nps_daily):\n",
    "    '''\n",
    "    Create the final unified dataset for modeling.\n",
    "    This is the foundation for all subsequent analysis.\n",
    "    '''\n",
    "\n",
    "    print(\"Creating Unified Dataset...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Start with hierarchy data\n",
    "    unified_data = hierarchy_data.copy()\n",
    "\n",
    "    # Merge marketing data\n",
    "    print(\"Merging marketing data...\")\n",
    "    marketing_cols = ['Date'] + marketing_channels\n",
    "    unified_data = unified_data.merge(\n",
    "        marketing_daily[marketing_cols],\n",
    "        on='Date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge NPS data\n",
    "    print(\"Merging NPS data...\")\n",
    "    unified_data = unified_data.merge(\n",
    "        nps_daily[['Date', 'NPS']],\n",
    "        on='Date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill any remaining NaN values\n",
    "    for col in marketing_channels:\n",
    "        if col in unified_data.columns:\n",
    "            unified_data[col] = unified_data[col].fillna(0)\n",
    "\n",
    "    unified_data['NPS'] = unified_data['NPS'].fillna(unified_data['NPS'].mean())\n",
    "\n",
    "    # Add time features for seasonality\n",
    "    unified_data['Year'] = unified_data['Date'].dt.year\n",
    "    unified_data['Month'] = unified_data['Date'].dt.month\n",
    "    unified_data['Quarter'] = unified_data['Date'].dt.quarter\n",
    "    unified_data['DayOfWeek'] = unified_data['Date'].dt.dayofweek\n",
    "    unified_data['WeekOfYear'] = unified_data['Date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    print(f\"\\nFinal Dataset Shape: {unified_data.shape}\")\n",
    "    print(f\"  Rows: {unified_data.shape[0]:,}\")\n",
    "    print(f\"  Columns: {unified_data.shape[1]}\")\n",
    "    print(f\"\\nFeatures included:\")\n",
    "    print(f\"  - Sales metrics: GMV, Units, Prices\")\n",
    "    print(f\"  - Marketing channels: {len(marketing_channels)}\")\n",
    "    print(f\"  - Brand health: NPS\")\n",
    "    print(f\"  - Time features: Year, Month, Quarter, etc.\")\n",
    "    print(f\"  - Product hierarchy: Category, Subcategory\")\n",
    "\n",
    "    return unified_data\n",
    "\n",
    "# Create unified dataset\n",
    "unified_data = create_unified_dataset(hierarchy_data, marketing_daily, nps_daily)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of Unified Dataset:\")\n",
    "display(unified_data.head())\n",
    "\n",
    "# Save the processed data\n",
    "unified_data.to_csv('data/processed/unified_mmm_data.csv', index=False)\n",
    "print(\"\\n[OK] Saved unified dataset to 'data/processed/unified_mmm_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5ca6c",
   "metadata": {},
   "source": [
    "## Step 6: Data Validation and Summary Statistics\n",
    "\n",
    "Let's validate our unified dataset and understand its characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_unified_dataset(unified_data):\n",
    "    '''\n",
    "    Comprehensive validation of the unified dataset.\n",
    "    Ensures data is ready for modeling.\n",
    "    '''\n",
    "\n",
    "    print(\"UNIFIED DATASET VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(\"\\n1. DATASET OVERVIEW:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total records: {len(unified_data):,}\")\n",
    "    print(f\"Date range: {unified_data['Date'].min()} to {unified_data['Date'].max()}\")\n",
    "    print(f\"Days covered: {unified_data['Date'].nunique()}\")\n",
    "    print(f\"Categories: {unified_data['product_category'].nunique()}\")\n",
    "    print(f\"Subcategories: {unified_data['product_subcategory'].nunique()}\")\n",
    "\n",
    "    # Sales statistics\n",
    "    print(\"\\n2. SALES METRICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total GMV: ${unified_data['GMV'].sum():,.0f}\")\n",
    "    print(f\"Average daily GMV: ${unified_data.groupby('Date')['GMV'].sum().mean():,.0f}\")\n",
    "    print(f\"Total units sold: {unified_data['Units'].sum():,.0f}\")\n",
    "\n",
    "    # Marketing statistics\n",
    "    print(\"\\n3. MARKETING INVESTMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    total_marketing = unified_data[marketing_channels].sum().sum()\n",
    "    print(f\"Total marketing spend: ${total_marketing:,.0f}\")\n",
    "\n",
    "    for channel in marketing_channels[:5]:  # Top 5 channels\n",
    "        if channel in unified_data.columns:\n",
    "            spend = unified_data[channel].sum()\n",
    "            pct = (spend / total_marketing) * 100\n",
    "            print(f\"  {channel}: ${spend:,.0f} ({pct:.1f}%)\")\n",
    "\n",
    "    # Data completeness\n",
    "    print(\"\\n4. DATA COMPLETENESS:\")\n",
    "    print(\"-\" * 40)\n",
    "    missing_pct = (unified_data.isnull().sum() / len(unified_data)) * 100\n",
    "    complete_features = (missing_pct == 0).sum()\n",
    "    print(f\"Features with no missing values: {complete_features}/{len(missing_pct)}\")\n",
    "\n",
    "    if missing_pct.sum() > 0:\n",
    "        print(\"\\nFeatures with missing values:\")\n",
    "        print(missing_pct[missing_pct > 0])\n",
    "\n",
    "    return True\n",
    "\n",
    "# Validate the dataset\n",
    "is_valid = validate_unified_dataset(unified_data)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"[OK] DATASET READY FOR MODELING!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b160e55",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Loaded three diverse data sources** with different granularities\n",
    "2. **Created hierarchical aggregation** to capture product-level patterns\n",
    "3. **Aligned time frequencies** through intelligent interpolation\n",
    "4. **Built a unified dataset** ready for feature engineering\n",
    "\n",
    "### Why This Matters for MMM:\n",
    "- **Hierarchical structure** allows us to model different product responses to marketing\n",
    "- **Daily granularity** captures immediate and lagged marketing effects\n",
    "- **Unified dataset** ensures consistent analysis across all components\n",
    "\n",
    "### Data Characteristics for Modeling:\n",
    "- **Response variable (Y):** GMV at category/subcategory level\n",
    "- **Marketing predictors (X):** 8+ marketing channels with daily spend\n",
    "- **Control variables:** Price, discount, NPS, seasonality\n",
    "- **Hierarchy:** Category and subcategory for pooled learning\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook, we'll engineer advanced features including:\n",
    "- Adstock transformations for marketing carryover\n",
    "- Beta-Gamma features for saturation curves\n",
    "- Price elasticity features\n",
    "- Seasonal decomposition\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** The quality of your data foundation determines the ceiling of your model's performance. We've built a solid foundation that preserves important patterns while maintaining data integrity."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}