{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658d9f8c",
   "metadata": {},
   "source": [
    "# Notebook 3: Building the Neural Additive Model Architecture\n",
    "## Designing Networks for Interpretable Marketing Mix Modeling\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand NAM architecture principles\n",
    "- Build feature-specific neural networks\n",
    "- Implement Beta-Gamma transformation layer\n",
    "- Apply monotonic constraints\n",
    "- Create the additive structure\n",
    "\n",
    "---\n",
    "\n",
    "## NAM Architecture Overview\n",
    "\n",
    "Unlike traditional neural networks, NAM builds a separate network for each feature, then combines them additively. This preserves interpretability while capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cac86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Load feature configuration from Notebook 2\n",
    "data = pd.read_csv('data/processed/mmm_data_with_features.csv')\n",
    "print(f\"Loaded data: {data.shape}\")\n",
    "\n",
    "# Identify feature types\n",
    "beta_gamma_features = [col for col in data.columns if any(x in col for x in ['_adstock', '_log'])]\n",
    "print(f\"Beta-Gamma features: {len(beta_gamma_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f44a43",
   "metadata": {},
   "source": [
    "## Part 1: The Beta-Gamma Transformation Layer\n",
    "\n",
    "This custom layer implements the marketing saturation curve:\n",
    "f(x) = alpha  x^beta  e^(-gammax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaGammaLayer(keras.layers.Layer):\n",
    "    '''\n",
    "    Custom layer implementing Beta-Gamma transformation for marketing saturation.\n",
    "\n",
    "    Parameters learned:\n",
    "    - alpha: Scale parameter (overall impact)\n",
    "    - beta: Shape parameter (initial effectiveness)\n",
    "    - gamma: Saturation parameter (diminishing returns)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable parameters\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=(1,),\n",
    "            initializer=keras.initializers.Constant(1.0),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=(1,),\n",
    "            initializer=keras.initializers.Constant(0.5),\n",
    "            constraint=keras.constraints.MinMaxNorm(min_value=0.1, max_value=2.0),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma',\n",
    "            shape=(1,),\n",
    "            initializer=keras.initializers.Constant(0.01),\n",
    "            constraint=keras.constraints.NonNeg(),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Ensure positive inputs\n",
    "        x = tf.nn.relu(inputs) + 1e-8\n",
    "\n",
    "        # Apply Beta-Gamma transformation\n",
    "        power_term = tf.pow(x, self.beta)\n",
    "        exp_term = tf.exp(-self.gamma * x)\n",
    "\n",
    "        return self.alpha * power_term * exp_term\n",
    "\n",
    "print(\"Beta-Gamma layer defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22eeb26",
   "metadata": {},
   "source": [
    "## Part 2: Feature-Specific Networks\n",
    "\n",
    "Each feature gets its own neural network based on its type:\n",
    "- **Marketing features**: Use Beta-Gamma transformation\n",
    "- **Price features**: Apply monotonic constraints\n",
    "- **Other features**: Standard neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_network(feature_name, feature_type):\n",
    "    '''\n",
    "    Create a feature-specific neural network based on feature type.\n",
    "    '''\n",
    "\n",
    "    # Single feature input\n",
    "    feature_input = keras.Input(shape=(1,), name=f'input_{feature_name}')\n",
    "\n",
    "    if 'beta_gamma' in feature_type:\n",
    "        # Marketing feature with saturation\n",
    "        hidden = layers.Dense(32, activation='relu')(feature_input)\n",
    "        hidden = layers.Dense(16, activation='relu')(hidden)\n",
    "        output = BetaGammaLayer(name=f'saturation_{feature_name}')(hidden)\n",
    "\n",
    "    elif 'monotonic_negative' in feature_type:\n",
    "        # Price feature (negative relationship)\n",
    "        hidden = layers.Dense(16, activation='relu')(feature_input)\n",
    "        positive_out = layers.Dense(1, activation='softplus',\n",
    "                                   kernel_constraint=keras.constraints.NonNeg())(hidden)\n",
    "        output = layers.Lambda(lambda x: -x, name=f'negative_{feature_name}')(positive_out)\n",
    "\n",
    "    elif 'monotonic_positive' in feature_type:\n",
    "        # Discount feature (positive relationship)\n",
    "        hidden = layers.Dense(16, activation='relu')(feature_input)\n",
    "        output = layers.Dense(1, activation='softplus',\n",
    "                            kernel_constraint=keras.constraints.NonNeg())(hidden)\n",
    "    else:\n",
    "        # Standard feature\n",
    "        hidden = layers.Dense(32, activation='relu')(feature_input)\n",
    "        hidden = layers.Dense(16, activation='relu')(hidden)\n",
    "        output = layers.Dense(1)(hidden)\n",
    "\n",
    "    return feature_input, output\n",
    "\n",
    "# Example: Create a marketing feature network\n",
    "example_input, example_output = create_feature_network('TV_adstock', 'beta_gamma')\n",
    "print(f\"Created feature network: Input shape {example_input.shape}, Output shape {example_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bd1e6",
   "metadata": {},
   "source": [
    "## Part 3: Building the Complete NAM Model\n",
    "\n",
    "Now we combine all feature networks into the additive structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nam_model(feature_names, feature_config):\n",
    "    '''\n",
    "    Build complete Neural Additive Model with feature-specific networks.\n",
    "    '''\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    print(\"Building NAM Model...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        # Determine feature type\n",
    "        if any(x in feature for x in ['_adstock', '_log']):\n",
    "            feature_type = 'beta_gamma'\n",
    "        elif 'Price' in feature or 'MRP' in feature:\n",
    "            feature_type = 'monotonic_negative'\n",
    "        elif 'Discount' in feature:\n",
    "            feature_type = 'monotonic_positive'\n",
    "        else:\n",
    "            feature_type = 'standard'\n",
    "\n",
    "        # Create feature network\n",
    "        feature_input, feature_output = create_feature_network(feature, feature_type)\n",
    "        inputs.append(feature_input)\n",
    "        outputs.append(feature_output)\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Created {i + 1} feature networks...\")\n",
    "\n",
    "    # Combine all features additively\n",
    "    if len(outputs) > 1:\n",
    "        combined = layers.Add(name='additive_combination')(outputs)\n",
    "    else:\n",
    "        combined = outputs[0]\n",
    "\n",
    "    # Final output layer\n",
    "    final_output = layers.Dense(1, name='prediction')(combined)\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=final_output, name='NAM_MMM')\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mape']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nModel built successfully!\")\n",
    "    print(f\"  Total parameters: {model.count_params():,}\")\n",
    "    print(f\"  Input features: {len(inputs)}\")\n",
    "    print(f\"  Beta-Gamma features: {sum(1 for f in feature_names if any(x in f for x in ['_adstock', '_log']))}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prepare features (excluding target and metadata)\n",
    "exclude_cols = ['Date', 'GMV', 'product_category', 'product_subcategory']\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "\n",
    "# Build the model (simplified for demo)\n",
    "print(f\"\\nBuilding model with {len(feature_cols)} features...\")\n",
    "# Note: In practice, you would pass actual features here\n",
    "# model = build_nam_model(feature_cols, feature_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd59f4",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Architecture\n",
    "\n",
    "Let's visualize how the NAM architecture differs from traditional neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_nam_architecture():\n",
    "    '''Visualize the NAM architecture concept'''\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Traditional Neural Network\n",
    "    ax = axes[0]\n",
    "    ax.text(0.5, 0.1, 'All Features', ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "    ax.arrow(0.5, 0.15, 0, 0.15, head_width=0.05, fc='black')\n",
    "    ax.text(0.5, 0.35, 'Hidden Layer 1', ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    ax.arrow(0.5, 0.4, 0, 0.15, head_width=0.05, fc='black')\n",
    "    ax.text(0.5, 0.6, 'Hidden Layer 2', ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    ax.arrow(0.5, 0.65, 0, 0.15, head_width=0.05, fc='black')\n",
    "    ax.text(0.5, 0.85, 'Output', ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Traditional NN (Black Box)', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # NAM Architecture\n",
    "    ax = axes[1]\n",
    "    features = ['TV', 'Price', 'Season', '...']\n",
    "    colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow']\n",
    "\n",
    "    for i, (feat, color) in enumerate(zip(features, colors)):\n",
    "        x = 0.2 + i * 0.2\n",
    "\n",
    "        # Feature\n",
    "        ax.text(x, 0.1, feat, ha='center', fontsize=9,\n",
    "                bbox=dict(boxstyle='round', facecolor=color))\n",
    "        ax.arrow(x, 0.15, 0, 0.25, head_width=0.02, fc='gray', alpha=0.5)\n",
    "\n",
    "        # Network\n",
    "        ax.text(x, 0.45, 'NN', ha='center', fontsize=8,\n",
    "                bbox=dict(boxstyle='round', facecolor=color, alpha=0.5))\n",
    "        ax.arrow(x, 0.5, 0, 0.15, head_width=0.02, fc='gray', alpha=0.5)\n",
    "\n",
    "        # Contribution\n",
    "        ax.text(x, 0.7, f'f({feat})', ha='center', fontsize=8)\n",
    "\n",
    "    # Addition\n",
    "    ax.text(0.5, 0.8, '+', ha='center', fontsize=14, fontweight='bold')\n",
    "    ax.arrow(0.5, 0.82, 0, 0.08, head_width=0.05, fc='black')\n",
    "    ax.text(0.5, 0.95, 'Output', ha='center', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('NAM (Interpretable)', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.suptitle('Neural Network Architectures Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_nam_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cec472",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We've Built:\n",
    "1. **Beta-Gamma Layer**: Custom transformation for marketing saturation\n",
    "2. **Feature Networks**: Separate neural network per feature\n",
    "3. **Monotonic Constraints**: Business logic enforcement\n",
    "4. **Additive Structure**: Interpretable combination\n",
    "\n",
    "### Architecture Benefits:\n",
    "- **Interpretability**: Can visualize each feature's contribution\n",
    "- **Business Logic**: Constraints ensure sensible predictions\n",
    "- **Flexibility**: Different architectures for different feature types\n",
    "- **Saturation Modeling**: Captures diminishing returns in marketing\n",
    "\n",
    "### The Power of NAM:\n",
    "- Unlike black-box models, we can see exactly how each feature contributes\n",
    "- Marketing features properly model saturation curves\n",
    "- Price relationships follow economic theory\n",
    "- The additive structure maintains interpretability\n",
    "\n",
    "### Next Steps:\n",
    "In Notebook 4, we'll train this model with proper validation techniques."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}