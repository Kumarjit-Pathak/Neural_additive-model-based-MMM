{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Neural Additive Models (NAM) - Complete Educational Tutorial\n",
    "\n",
    "**Welcome!** This notebook teaches you how to build, train, and interpret Neural Additive Models.\n",
    "\n",
    "## üéØ Learning Objectives:\n",
    "1. Understand NAM architecture and why it's explainable\n",
    "2. Load and process daily sales data (250 records)\n",
    "3. Build a single-layer NAM for interpretability\n",
    "4. Train with proper validation strategies\n",
    "5. Visualize predictions and elasticity curves\n",
    "6. Decompose predictions into business drivers\n",
    "\n",
    "## üìä What Makes NAM Special?\n",
    "\n",
    "**Traditional Neural Networks:**\n",
    "```\n",
    "y = NN(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)  # Black box!\n",
    "```\n",
    "\n",
    "**Neural Additive Models:**\n",
    "```\n",
    "y = f‚ÇÅ(x‚ÇÅ) + f‚ÇÇ(x‚ÇÇ) + ... + f‚Çô(x‚Çô)  # Explainable!\n",
    "```\n",
    "\n",
    "Each feature has its **own neural network**, and predictions are **summed**.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Can plot individual feature contribution curves\n",
    "- ‚úÖ No feature interactions (easier to explain)\n",
    "- ‚úÖ Business-friendly interpretations\n",
    "- ‚úÖ Regulatory compliant (explainable AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup - Run this first!\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path('.').absolute() / 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"‚úì Environment setup complete!\")\n",
    "print(\"Using Keras backend: JAX\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Load Daily Sales Data\n",
    "\n",
    "**Key Insight:** Daily data provides 20x more samples than monthly aggregation!\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "Sales.csv (1M+ transactions)\n",
    "    ‚Üì Parse dates\n",
    "    ‚Üì Aggregate by day\n",
    "    ‚Üì Result: 250 daily records\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "# Load daily data\n",
    "loader = DataLoader('data/raw')\n",
    "daily_data = loader.load_daily_sales()\n",
    "\n",
    "print(f\"üìä Daily Data Loaded:\")\n",
    "print(f\"   Records: {len(daily_data):,}\")\n",
    "print(f\"   Date range: {daily_data['Date'].min().date()} to {daily_data['Date'].max().date()}\")\n",
    "print(f\"   Columns: {len(daily_data.columns)}\")\n",
    "\n",
    "# Show first few rows\n",
    "daily_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Section 2: Feature Engineering & Scaling\n",
    "\n",
    "**Critical Steps:**\n",
    "1. Log-transform large values (GMV, investment)\n",
    "2. StandardScaler for all features\n",
    "3. Drop raw (unscaled) columns\n",
    "\n",
    "**Why?** Neural networks need features in similar scales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.data.data_preprocessing import DataPreprocessor\n",
    "from src.data.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = DataPreprocessor({})\n",
    "data = preprocessor.handle_missing_values(daily_data)\n",
    "data = preprocessor.treat_outliers(data)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = FeatureEngineer({})\n",
    "data = engineer.engineer_all_features(data)\n",
    "\n",
    "print(f\"‚úì Feature engineering complete\")\n",
    "print(f\"   Total columns: {len(data.columns)}\")\n",
    "\n",
    "# Scaling\n",
    "data_scaled, scalers = preprocessor.scale_features(data)\n",
    "\n",
    "print(f\"‚úì Feature scaling complete\")\n",
    "print(f\"   All features in range [-3, +3]\")\n",
    "\n",
    "# Show distribution\n",
    "numeric_cols = data_scaled.select_dtypes(include=[np.number]).columns\n",
    "print(f\"\\nüìä Scaled Features ({len(numeric_cols)}):\")\n",
    "for col in list(numeric_cols)[:5]:\n",
    "    print(f\"   {col}: [{data_scaled[col].min():.2f}, {data_scaled[col].max():.2f}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 3: Train/Val/Test Split (Time Series)\n",
    "\n",
    "**Important:** For time series, we NEVER shuffle!\n",
    "\n",
    "**Split Strategy:**\n",
    "- Train: 70% (175 days)\n",
    "- Val: 15% (37 days)\n",
    "- Test: 15% (38 days)\n",
    "\n",
    "**Result:** 38 test days for **clear trend visualization**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Time series split\n",
    "train_size = int(len(data_scaled) * 0.70)\n",
    "val_size = int(len(data_scaled) * 0.15)\n",
    "\n",
    "train_data = data_scaled.iloc[:train_size]\n",
    "val_data = data_scaled.iloc[train_size:train_size+val_size]\n",
    "test_data = data_scaled.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Train: {len(train_data)} days\")\n",
    "print(f\"   Val:   {len(val_data)} days\")\n",
    "print(f\"   Test:  {len(test_data)} days\")\n",
    "print(f\"\\n‚úì Statistical Power:\")\n",
    "print(f\"   Samples per feature: {len(train_data) / 9:.1f} (EXCELLENT!)\")\n",
    "print(f\"   vs Monthly: 0.27 samples/feature (POOR)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 4: Build Single-Layer NAM\n",
    "\n",
    "**Architecture Choice:** Single layer [16] for explainability\n",
    "\n",
    "**Structure:**\n",
    "```python\n",
    "For each feature i:\n",
    "    f·µ¢(x·µ¢) = Dense(16, relu)(x·µ¢) ‚Üí Dense(1)(¬∑)\n",
    "\n",
    "Final prediction = Œ£ f·µ¢(x·µ¢)\n",
    "```\n",
    "\n",
    "**Parameters:** Only 441 total (highly interpretable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.models.simple_nam import SimpleNAM\n",
    "from src.training.trainer import NAMTrainer\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = NAMTrainer.prepare_data_for_keras(train_data)\n",
    "\n",
    "print(f\"üìä Prepared Data:\")\n",
    "print(f\"   X shape: {X_train.shape}\")\n",
    "print(f\"   y shape: {y_train.shape}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Build model\n",
    "model = SimpleNAM(\n",
    "    n_features=X_train.shape[1],\n",
    "    feature_types=['unconstrained'] * X_train.shape[1],\n",
    "    hidden_dims=[16]  # Single layer!\n",
    ")\n",
    "\n",
    "# Build\n",
    "_ = model(X_train[:1])\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Model Built:\")\n",
    "print(f\"   Architecture: Single-layer NAM\")\n",
    "print(f\"   Parameters: {model.count_params():,}\")\n",
    "print(f\"   Explainability: HIGH ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ\")\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Section 5: Train the Model\n",
    "\n",
    "**Training Strategy:**\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Early stopping (patience=30)\n",
    "- Learning rate reduction on plateau\n",
    "- Model checkpointing\n",
    "\n",
    "**Watch the convergence!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.utils.config import load_config\n",
    "\n",
    "# Load training config\n",
    "training_config = load_config('configs/training_config.yaml')\n",
    "\n",
    "# Create trainer\n",
    "trainer = NAMTrainer(model, training_config['training'])\n",
    "\n",
    "# Train!\n",
    "print(\"üöÄ Starting training...\")\n",
    "history = trainer.train(train_data, val_data, epochs=50)\n",
    "\n",
    "print(f\"\\n‚úì Training complete!\")\n",
    "print(f\"   Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"   Total epochs: {len(history.history['loss'])}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Section 6: Visualize Training (Interactive!)\n",
    "\n",
    "**Interactive Plotly charts** - zoom, pan, hover for details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create interactive training history\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Training & Validation Loss', 'Mean Absolute Error')\n",
    ")\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(epochs), y=history.history['loss'], name='Train Loss',\n",
    "               line=dict(color='#2E86AB', width=3)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(epochs), y=history.history['val_loss'], name='Val Loss',\n",
    "               line=dict(color='#A23B72', width=3)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# MAE curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(epochs), y=history.history['mae'], name='Train MAE',\n",
    "               line=dict(color='#2E86AB', width=3)),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(epochs), y=history.history['val_mae'], name='Val MAE',\n",
    "               line=dict(color='#A23B72', width=3)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, title_text=\"Training History (Interactive - Try Hovering!)\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"üí° Hover over the lines to see exact values!\")\n",
    "print(f\"üí° Double-click legend to isolate a curve!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 7: Make Predictions on Test Set (38 Days!)\n",
    "\n",
    "**This shows the complete time series trend - a key advantage of daily data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get test predictions\n",
    "X_test, y_test = NAMTrainer.prepare_data_for_keras(test_data)\n",
    "predictions = model.predict(X_test).flatten()\n",
    "\n",
    "# Inverse transform (if needed)\n",
    "# For simplicity, we'll use scaled values here\n",
    "# (See main_daily.py for full inverse transform)\n",
    "\n",
    "test_dates = test_data['Date'].values\n",
    "\n",
    "print(f\"üìä Test Predictions:\")\n",
    "print(f\"   Test samples: {len(predictions)}\")\n",
    "print(f\"   Date range: {test_dates[0]} to {test_dates[-1]}\")\n",
    "print(f\"   ‚úì Complete trend visible with {len(predictions)} daily points!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interactive time series visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Actual\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_dates,\n",
    "    y=y_test,\n",
    "    mode='lines+markers',\n",
    "    name='Actual',\n",
    "    line=dict(color='#2E86AB', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='<b>Actual</b><br>Date: %{x}<br>GMV: %{y:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Predicted\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=test_dates,\n",
    "    y=predictions,\n",
    "    mode='lines+markers',\n",
    "    name='Predicted',\n",
    "    line=dict(color='#A23B72', width=2, dash='dash'),\n",
    "    marker=dict(size=6, symbol='square'),\n",
    "    hovertemplate='<b>Predicted</b><br>Date: %{x}<br>GMV: %{y:.3f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='38-Day Test Period: Actual vs Predicted (Interactive!)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='GMV (Scaled)',\n",
    "    height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Try these interactions:\")\n",
    "print(\"   - Hover to see exact values\")\n",
    "print(\"   - Click and drag to zoom\")\n",
    "print(\"   - Double-click to reset zoom\")\n",
    "print(\"   - Click legend to show/hide series\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Section 8: Calculate Advanced Metrics\n",
    "\n",
    "**Beyond R¬≤ and MAPE - industry-standard KPIs!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.evaluation.advanced_metrics import compute_all_metrics\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(y_test, predictions)\n",
    "\n",
    "# Display as table\n",
    "metrics_df = pd.DataFrame([\n",
    "    {'Metric': 'R¬≤ Score', 'Value': f\"{metrics['r2']:.4f}\"},\n",
    "    {'Metric': 'MAE', 'Value': f\"{metrics['mae']:.4f}\"},\n",
    "    {'Metric': 'RMSE', 'Value': f\"{metrics['rmse']:.4f}\"},\n",
    "    {'Metric': 'MAPE', 'Value': f\"{metrics['mape']:.2f}%\"},\n",
    "    {'Metric': 'Weighted MAPE', 'Value': f\"{metrics['wmape']:.2f}%\"},\n",
    "    {'Metric': 'Symmetric MAPE', 'Value': f\"{metrics['smape']:.2f}%\"},\n",
    "    {'Metric': 'Bias %', 'Value': f\"{metrics['bias_pct']:.2f}%\"},\n",
    "])\n",
    "\n",
    "print(\"üìä Comprehensive Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Highlight\n",
    "if metrics['r2'] > 0.3:\n",
    "    print(f\"\\n‚úì R¬≤ = {metrics['r2']:.3f} indicates good learning!\")\n",
    "if metrics['smape'] < 50:\n",
    "    print(f\"‚úì sMAPE = {metrics['smape']:.1f}% is acceptable for forecasting!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Section 9: NAM Explainability - Feature Contributions\n",
    "\n",
    "**The POWER of NAM:** We can extract how much each feature contributes!\n",
    "\n",
    "**Example:** \"Price contributed -$X to GMV\" vs \"Marketing contributed +$Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract feature contributions\n",
    "if hasattr(model, 'get_feature_contributions'):\n",
    "    contributions = model.get_feature_contributions(X_test)\n",
    "    \n",
    "    # Average contribution per feature\n",
    "    feature_names = [col for col in data_scaled.select_dtypes(include=[np.number]).columns \n",
    "                     if col != 'total_gmv_log']\n",
    "    \n",
    "    avg_contributions = {}\n",
    "    for i, contrib in enumerate(contributions[:len(feature_names)]):\n",
    "        avg_contributions[feature_names[i]] = np.mean(contrib)\n",
    "    \n",
    "    # Plot\n",
    "    contrib_df = pd.DataFrame(list(avg_contributions.items()), \n",
    "                              columns=['Feature', 'Contribution'])\n",
    "    contrib_df = contrib_df.sort_values('Contribution', ascending=True)\n",
    "    \n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=contrib_df['Contribution'],\n",
    "        y=contrib_df['Feature'],\n",
    "        orientation='h',\n",
    "        marker=dict(color=contrib_df['Contribution'], \n",
    "                   colorscale='RdBu', cmid=0)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Average Feature Contributions',\n",
    "        xaxis_title='Contribution to GMV',\n",
    "        yaxis_title='Feature',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nüí° This is NAM's interpretability advantage!\")\n",
    "    print(\"   Red bars: Positive contribution\")\n",
    "    print(\"   Blue bars: Negative contribution\")\n",
    "else:\n",
    "    print(\"Note: Feature contribution extraction available in full model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Section 10: Elasticity Curves\n",
    "\n",
    "**Business Question:** \"How does GMV change if I adjust price by 10%?\"\n",
    "\n",
    "**NAM Answer:** Plot the learned curve and find optimal point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract elasticity for a feature\n",
    "def plot_feature_elasticity(model, X_baseline, feature_idx, feature_name):\n",
    "    \"\"\"Plot how GMV changes as we vary one feature\"\"\"\n",
    "    \n",
    "    # Vary feature from -3 to +3 (scaled range)\n",
    "    feature_values = np.linspace(-3, 3, 100)\n",
    "    predictions_range = []\n",
    "    \n",
    "    for val in feature_values:\n",
    "        X_test = X_baseline.copy()\n",
    "        X_test[:, feature_idx] = val\n",
    "        pred = model.predict(X_test, verbose=0).flatten()[0]\n",
    "        predictions_range.append(pred)\n",
    "    \n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=feature_values,\n",
    "        y=predictions_range,\n",
    "        mode='lines',\n",
    "        name='Elasticity Curve',\n",
    "        line=dict(color='#06A77D', width=4)\n",
    "    ))\n",
    "    \n",
    "    # Mark current value\n",
    "    current_val = X_baseline[0, feature_idx]\n",
    "    current_pred = predictions_range[np.argmin(np.abs(feature_values - current_val))]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[current_val],\n",
    "        y=[current_pred],\n",
    "        mode='markers',\n",
    "        name='Current Point',\n",
    "        marker=dict(size=15, color='red', symbol='star')\n",
    "    ))\n",
    "    \n",
    "    # Mark optimal\n",
    "    optimal_idx = np.argmax(predictions_range)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[feature_values[optimal_idx]],\n",
    "        y=[predictions_range[optimal_idx]],\n",
    "        mode='markers',\n",
    "        name='Optimal Point',\n",
    "        marker=dict(size=15, color='gold', symbol='diamond')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Elasticity Curve: {feature_name}',\n",
    "        xaxis_title=f'{feature_name} (Scaled)',\n",
    "        yaxis_title='GMV Contribution',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example: Plot elasticity for first feature\n",
    "X_baseline = np.median(X_train, axis=0, keepdims=True)\n",
    "feature_names = [col for col in data_scaled.select_dtypes(include=[np.number]).columns \n",
    "                 if col != 'total_gmv_log']\n",
    "\n",
    "if len(feature_names) > 0:\n",
    "    fig = plot_feature_elasticity(model, X_baseline, 0, feature_names[0])\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\nüí° Interpretation:\")\n",
    "    print(f\"   - Red star: Current feature value\")\n",
    "    print(f\"   - Gold diamond: Optimal value for max GMV\")\n",
    "    print(f\"   - Curve shape: How GMV responds to this feature\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Section 11: Student Exercises\n",
    "\n",
    "**Try These:**\n",
    "\n",
    "1. **Experiment with architecture:**\n",
    "   - Try `hidden_dims=[8]` (simpler) or `[32]` (more complex)\n",
    "   - Compare R¬≤ scores\n",
    "\n",
    "2. **Different train/test splits:**\n",
    "   - Try 60/20/20 split\n",
    "   - How does it affect performance?\n",
    "\n",
    "3. **Feature importance:**\n",
    "   - Extract contributions for all features\n",
    "   - Rank by importance\n",
    "   - Which drives GMV most?\n",
    "\n",
    "4. **Elasticity analysis:**\n",
    "   - Plot curves for all features\n",
    "   - Find optimal points\n",
    "   - Calculate revenue impact\n",
    "\n",
    "5. **Walk-forward validation:**\n",
    "   - Enable in config\n",
    "   - Run with 10-day holdouts\n",
    "   - Analyze robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**What You Learned:**\n",
    "1. ‚úÖ NAM provides interpretability via additive structure\n",
    "2. ‚úÖ Daily data (250 records) >> Monthly data (12 records)\n",
    "3. ‚úÖ Single-layer [16] balances explainability & performance\n",
    "4. ‚úÖ Proper scaling is critical (log + StandardScaler)\n",
    "5. ‚úÖ 38 test points give clear trend visualization\n",
    "6. ‚úÖ Can extract feature contributions and elasticities\n",
    "\n",
    "**Why NAM for Business:**\n",
    "- Explainable predictions (regulatory compliance)\n",
    "- Feature contribution curves (investment decisions)\n",
    "- Elasticity analysis (pricing optimization)\n",
    "- No black-box problem (stakeholder trust)\n",
    "\n",
    "**Next Steps:**\n",
    "- Try exercises above\n",
    "- Explore `main_daily.py` for production code\n",
    "- Read `FINAL_SUMMARY.md` for complete system details\n",
    "- Check `START_HERE.md` for quick reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Additional Resources\n",
    "\n",
    "**Documentation in this repo:**\n",
    "- `START_HERE.md` - Quick start guide\n",
    "- `FINAL_SUMMARY.md` - Complete technical summary\n",
    "- `HOW_TO_RUN_VISUALIZATIONS.md` - Visualization guide\n",
    "- `INTERACTIVE_VISUALIZATION_GUIDE.md` - Plotly dashboards\n",
    "\n",
    "**Academic Papers:**\n",
    "- Agarwal et al. (2021) \"Neural Additive Models: Interpretable Machine Learning with Neural Nets\"\n",
    "- Original NAM paper from Google Research\n",
    "\n",
    "**Questions?** Check the documentation or experiment with the code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
